# 相关概念

内存管理相关概念

## DMA(Direct Memory Access)

DMA即直接内存访问，是一种硬件机制，允许外设在无需CPU参与条件下，直接与内存进行数据传输。这种方式能减少CPU负担，提高数据吞吐量，尤其适用于大数据量传输的场景。

一般来说，Linux内核启动时候会将物理内存划分为不同的区域，一般有：`ZONE_DMA`(低地址<16MB，供DMA设备使用)、`ZONE_NORMAL`(常规地址, 直接映射到内核地址空间)、`ZONE_HIGHMEM`(高地址，不直接映射，需通过kmap访问，仅32位系统使用)

### 无DMA情况下数据传输

1. CPU从设备读取数据到寄存器
2. 再从寄存器写入内存(或反之)

### 使用DMA情况下数据传输

1. 设备向DMA控制器(DMAC)发出请求
2. DMAC直接在内存和设备之间传输数据
3. 传输完成后，DMAC通知CPU(通过中断)
4. CPU只需在传输开始和结束时候介入，极大提高效率

### 常见DMA设备

1. 存储设备：硬盘(HDD、SSD)，使用DMA或DME(Direct Memory Access Engine)进行数据读写，避免CPU处理大量数据搬运；NVME SSD使用PCIe总线的DMA机制，实现超高速据传输；
2. 网络设备：网卡(NIC)，现代网卡(如：10GbE、RDMA网卡)使用DMA直接将数据存入内存，加速网络数据处理(如：Zero-Copy技术)
3. InfiniBand和RDMA(远程直接内存访问)设备：高性能计算(HPC)中常见，允许数据直接从一个计算节点的内存传输到另一个节点的内存
4. 图形处理单元(GPU)：显卡(GPU)，数据从系统内存传输到显存(VRAM)时，通常使用DMA，如PCIe总线上的DMA传输。CUDA/OpenCL计算：GPU计算时候，数据通过DMA传输，减少CPU负担。
5. 音频设备：声卡；USB音频设备，使用USB DMA机制，避免CPU直接搬运数据
6. 工业和嵌入式设备：嵌入式SoC、FPGA设备，许多FPGA通过PCIe DMA直接与主机系统交互，适用于高吞吐量数据处理(如：信号处理、AI推理加速)

### DMA内存使用

老旧DMA设备，尤其是ISA(Industry Standard Architecture)总线设备，它们只能访问低于16MB的物理地址空间(因为只使用了24位地址线)。

现代DMA设备，如PCI/PCIe设备、NVMe、RDMA网卡等，能够访问整个物理地址空间，甚至支持IOMMU(I/O Memory Management Unit)进行地址翻译，不受16MB限制。

在Linux内存管理中：
- `ZONE_DMA(0-16MB)`：为老旧ISA设备保留DMA访问区域
- `ZONE_DMA32(0-4GB)`：为32位PCI设备提供DMA访问支持
- `ZONE_NORMAL`：用于现代64位设备，可以直接访问更大地址空间

### DMA设备的工作流程

#### 1. 初始化DMA传输

CPU配置DMA控制器(DMAC)或DMA-capable设备：
- 设置源地址：数据的起始位置(内存地址/设备地址)
- 设置目标地址：数据要传输到的位置(内存地址/设备地址)
- 设置传输大小：
- 选择传输模式（如：单次传输、突发模式、循环模式等）

#### 2. DMA控制器启动传输

- cpu 触发DMA设备启动传输，然后CPU可以做其他任务
- DMA设备或控制器直接读取或写入内存，不经过CPU。

#### 3. DMA设备执行数据搬运

- 读取阶段：
    - 如果是设备 -> 内存传输(如：硬盘数据到RAM)：
        - 设备发送请求到DMA控制器
        - DMA控制器从设备缓存区读取数据
        - DMA控制器将数据存入内存的目标地址
    - 如果是内存 -> 设备传输(如网卡发送数据)：
        - DMA控制器从内存读取数据块
        - 数据被传输到设备的I/O端口或缓存区
- 传输完成&通知CPU：
    - 传输完成后，DMA设备向CPU发出中断(IRQ)，通知数据已经传输完毕
    - CPU处理中断，检查数据完整性，并执行后续操作(如：用户态程序读取数据)

### DMA传输模式

|传输模式|说明|适用场景|
|:-------|:---|:-------|
|单字节模式|一次传输1个字节/字(CPU需要多次介入)|低速设备，如：UART|
|突发模式|一次传输多个字节|硬盘、网络|
|块模式|传输一个完整数据块，无需CPU介入|高速设备、如：SSD|
|循环模式|使用双缓存，提高并行性|实时语音处理、视频采集|

### 现代DMA技术

- IOMMU(I/O Memory Management Unit)：现代操作系统(如：Linux)使用IOMMU让DMA设备能访问超过4GB地址空间，并提供地址映射和安全隔离
- RDMA(Remote Direct Memory Access)：允许服务器之间直接通过网络进行DMA传输，不经过CPU，提高分布式计算性能(如：HPC、高速数据库)

## NUMA内存管理

NUMA(Non-Uniform Memory Access)架构的内存管理涉及多个方面，包括内存分配策略、拓扑感知和调度优化。

### NUMA内存管理

#### 内存分配策略

- First-Touch(首次触碰分配)：线程首次访问某个内存页时候，操作系统将其分配到该线程所在NUMA节点的内存中。
- Local Allocation(本地优先分配)：尽量在当前CPU所在的NUMA节点分配内存，以减少远程访问延迟。
- Interleaved(交错分配)：在多个NUMA节点之间均匀分配内存，提高带宽利用率(numactl --interleave=all)
- Explicit Binding(显示绑定)：使用`mbind()`或`numactl --membind`指定内存分配到特定NUMA节点

#### NUMA负载均衡

内核会定期检查NUMA节点的负载情况，并在必要时候执行：
- 自动迁移(Auto Migration)：将进程的内存页迁移到与其运行CPU更接近的NUMA节点
- 复制热点页(Page Replcation)：对于共享访问的页，可能会在多个NUMA节点上复制，以减少跨 NUMA 访问开销

#### 进程/线程调度

- NUMA感知调度(NUMA-Aware Scheduling)：尽量将线程绑定到靠近其分配内存的CPU上
- 使用`sched_setaffinity()`或`taskset`指定进程运行在特定NUMA节点的CPU上

### 处理器与内存的距离划分

NUMA系统中的处理器与内存的距离划分通常由硬件拓扑决定，可以通过以下方法查看：

#### 使用numactl工具

```shell
numactl --hardware
```

输出示例：
```
available: 2 nodes (0-1)
node 0 cpus: 0-15
node 0 size: 128000 MB
node 1 cpus: 16-31
node 1 size: 128000 MB
node distances:
node    0    1
0:     10   21
1:     21   10
```
这里的distance矩阵表示访问延迟，数值越小表示访问更快。比如：
- 本地访问(node 0 自己的内存)开销是10
- 远程访问(node 0 访问 node 1 的内存)开销是 21

#### 使用numastat查看NUMA访问统计

```sh
numastat -p <进程名>
```

可以检查进程的内存访问模式，识别是否由大量的远程访问
