# 相关概念

内存管理相关概念

## DMA(Direct Memory Access)

DMA即直接内存访问，是一种硬件机制，允许外设在无需CPU参与条件下，直接与内存进行数据传输。这种方式能减少CPU负担，提高数据吞吐量，尤其适用于大数据量传输的场景。

一般来说，Linux内核启动时候会将物理内存划分为不同的区域，一般有：`ZONE_DMA`(低地址<16MB，供DMA设备使用)、`ZONE_NORMAL`(常规地址, 直接映射到内核地址空间)、`ZONE_HIGHMEM`(高地址，不直接映射，需通过kmap访问，仅32位系统使用)

### 无DMA情况下数据传输

1. CPU从设备读取数据到寄存器
2. 再从寄存器写入内存(或反之)

### 使用DMA情况下数据传输

1. 设备向DMA控制器(DMAC)发出请求
2. DMAC直接在内存和设备之间传输数据
3. 传输完成后，DMAC通知CPU(通过中断)
4. CPU只需在传输开始和结束时候介入，极大提高效率

### 常见DMA设备

1. 存储设备：硬盘(HDD、SSD)，使用DMA或DME(Direct Memory Access Engine)进行数据读写，避免CPU处理大量数据搬运；NVME SSD使用PCIe总线的DMA机制，实现超高速据传输；
2. 网络设备：网卡(NIC)，现代网卡(如：10GbE、RDMA网卡)使用DMA直接将数据存入内存，加速网络数据处理(如：Zero-Copy技术)
3. InfiniBand和RDMA(远程直接内存访问)设备：高性能计算(HPC)中常见，允许数据直接从一个计算节点的内存传输到另一个节点的内存
4. 图形处理单元(GPU)：显卡(GPU)，数据从系统内存传输到显存(VRAM)时，通常使用DMA，如PCIe总线上的DMA传输。CUDA/OpenCL计算：GPU计算时候，数据通过DMA传输，减少CPU负担。
5. 音频设备：声卡；USB音频设备，使用USB DMA机制，避免CPU直接搬运数据
6. 工业和嵌入式设备：嵌入式SoC、FPGA设备，许多FPGA通过PCIe DMA直接与主机系统交互，适用于高吞吐量数据处理(如：信号处理、AI推理加速)

### DMA内存使用

老旧DMA设备，尤其是ISA(Industry Standard Architecture)总线设备，它们只能访问低于16MB的物理地址空间(因为只使用了24位地址线)。

现代DMA设备，如PCI/PCIe设备、NVMe、RDMA网卡等，能够访问整个物理地址空间，甚至支持IOMMU(I/O Memory Management Unit)进行地址翻译，不受16MB限制。

在Linux内存管理中：
- `ZONE_DMA(0-16MB)`：为老旧ISA设备保留DMA访问区域
- `ZONE_DMA32(0-4GB)`：为32位PCI设备提供DMA访问支持
- `ZONE_NORMAL`：用于现代64位设备，可以直接访问更大地址空间

### DMA设备的工作流程

#### 1. 初始化DMA传输

CPU配置DMA控制器(DMAC)或DMA-capable设备：
- 设置源地址：数据的起始位置(内存地址/设备地址)
- 设置目标地址：数据要传输到的位置(内存地址/设备地址)
- 设置传输大小：
- 选择传输模式（如：单次传输、突发模式、循环模式等）

#### 2. DMA控制器启动传输

- cpu 触发DMA设备启动传输，然后CPU可以做其他任务
- DMA设备或控制器直接读取或写入内存，不经过CPU。

#### 3. DMA设备执行数据搬运

- 读取阶段：
    - 如果是设备 -> 内存传输(如：硬盘数据到RAM)：
        - 设备发送请求到DMA控制器
        - DMA控制器从设备缓存区读取数据
        - DMA控制器将数据存入内存的目标地址
    - 如果是内存 -> 设备传输(如网卡发送数据)：
        - DMA控制器从内存读取数据块
        - 数据被传输到设备的I/O端口或缓存区
- 传输完成&通知CPU：
    - 传输完成后，DMA设备向CPU发出中断(IRQ)，通知数据已经传输完毕
    - CPU处理中断，检查数据完整性，并执行后续操作(如：用户态程序读取数据)

### DMA传输模式

|传输模式|说明|适用场景|
|:-------|:---|:-------|
|单字节模式|一次传输1个字节/字(CPU需要多次介入)|低速设备，如：UART|
|突发模式|一次传输多个字节|硬盘、网络|
|块模式|传输一个完整数据块，无需CPU介入|高速设备、如：SSD|
|循环模式|使用双缓存，提高并行性|实时语音处理、视频采集|

### 现代DMA技术

- IOMMU(I/O Memory Management Unit)：现代操作系统(如：Linux)使用IOMMU让DMA设备能访问超过4GB地址空间，并提供地址映射和安全隔离
- RDMA(Remote Direct Memory Access)：允许服务器之间直接通过网络进行DMA传输，不经过CPU，提高分布式计算性能(如：HPC、高速数据库)

## NUMA内存管理

NUMA(Non-Uniform Memory Access)架构的内存管理涉及多个方面，包括内存分配策略、拓扑感知和调度优化。

### NUMA内存管理

#### 内存分配策略

- First-Touch(首次触碰分配)：线程首次访问某个内存页时候，操作系统将其分配到该线程所在NUMA节点的内存中。
- Local Allocation(本地优先分配)：尽量在当前CPU所在的NUMA节点分配内存，以减少远程访问延迟。
- Interleaved(交错分配)：在多个NUMA节点之间均匀分配内存，提高带宽利用率(numactl --interleave=all)
- Explicit Binding(显示绑定)：使用`mbind()`或`numactl --membind`指定内存分配到特定NUMA节点

#### NUMA负载均衡

内核会定期检查NUMA节点的负载情况，并在必要时候执行：
- 自动迁移(Auto Migration)：将进程的内存页迁移到与其运行CPU更接近的NUMA节点
- 复制热点页(Page Replcation)：对于共享访问的页，可能会在多个NUMA节点上复制，以减少跨 NUMA 访问开销

#### 进程/线程调度

- NUMA感知调度(NUMA-Aware Scheduling)：尽量将线程绑定到靠近其分配内存的CPU上
- 使用`sched_setaffinity()`或`taskset`指定进程运行在特定NUMA节点的CPU上

### 处理器与内存的距离划分

NUMA系统中的处理器与内存的距离划分通常由硬件拓扑决定，可以通过以下方法查看：

#### 使用numactl工具

```shell
numactl --hardware
```

输出示例：
```
available: 2 nodes (0-1)
node 0 cpus: 0-15
node 0 size: 128000 MB
node 1 cpus: 16-31
node 1 size: 128000 MB
node distances:
node    0    1
0:     10   21
1:     21   10
```
这里的distance矩阵表示访问延迟，数值越小表示访问更快。比如：
- 本地访问(node 0 自己的内存)开销是10
- 远程访问(node 0 访问 node 1 的内存)开销是 21

#### 使用numastat查看NUMA访问统计

```sh
numastat -p <进程名>
```

可以检查进程的内存访问模式，识别是否由大量的远程访问

## struct memblock 结构体

在Linux内核中，`struct memblock`主要用于在内核早期启动阶段记录和管理物理内存的布局信息。具体来说，它的作用包括：
- 记录内存区域：memblock保存了来自固件(例如: BIOS或设备树)的物理内存信息，记录了哪些区域是可用的，哪些区域是预留给硬件或内核自身的，它维护了两个主要的列表：
    - 可用内存区域列表
    - 保留内存区域列表
- 早期内存分配：在内核建立起完善的内存管理子系统(如：页分配器和buddy系统)之前，memblock提供了一个简单的内存分配接口，用于满足启动阶段的内存分配需求。内核通过调用memblock分配函数来分配早期所需的内存，确保在后续内存管理器值初始化之前，内核有足够的内存空间用于初始化工作
- 作为后续内存管理数据的基础：memblock中记录的内存信息最终会被用来构建更为复杂和高效的内存管理结构，例如：页描述符数组和buddy分配器。`mem_init()`等初始化函数会从memblock中获取内存区域信息，金额日进行细致的内存区域划分(比如：划分zone、NUMA节点等)

`enum memblock_flags`，定义了内存区域属性：
```c
enum memblock_flags
{
    MEMBLOCK_NONE           = 0x0,  /* No special request。常规可用内存 */
    MEMBLOCK_HOTPLUG        = 0x1,  /* hotpluggable region。内存支持热插拔，运行时可添加、删除 */
    MEMBLOCK_MIRROR         = 0x2,  /* mirrored region。内存镜像区域，用于冗余或调试 */
    MEMBLOCK_NOMAP          = 0x4,  /* don't add to kernel direct mapping。内存不参与内核直接映射，常用于预留或特殊用途 */
    MEMBLOCK_DRIVER_MANAGED = 0x8,  /* always detected via a driver。总是通过驱动程序检测和添加的内存区域，
                                        在固件提供的内存映射中从未显示为系统 RAM。
                                        这与内核资源树中的 IORESOURCE_SYSRAM_DRIVER_MANAGED 相对应。*/
    MEMBLOCK_RSRV_NOINIT    = 0x10, /* don't initialize struct pages。 结构页未初始化的内存区域
                                        （仅适用于保留区域）。 */
};
```

`struct memblock_region`，表示一个内存区域：

```c
/**
 * struct memblock_region - represents a memory region
 * @base: base address of the region
 * @size: size of the region
 * @flags: memory region attributes
 * @nid: NUMA node id
 */
struct memblock_region
{
    phys_addr_t         base;   // 区域起始地址
    phys_addr_t         size;   // 区域大小
    enum memblock_flags flags;  // 区域属性
#ifdef CONFIG_NUMA
    int nid;                    // NUMA 节点编号
#endif
};
```

`struct memblock_type`，表示一类内存区域的集合：
```c
/**
 * struct memblock_type - collection of memory regions of certain type
 * @cnt: number of regions
 * @max: size of the allocated array
 * @total_size: size of all regions
 * @regions: array of regions
 * @name: the memory type symbolic name
 */
struct memblock_type
{
    unsigned long           cnt;            // 当前已记录内存区域的数量
    unsigned long           max;            // 数组可容纳最大内存区域数量
    phys_addr_t             total_size;     // 所有区域的总大小
    struct memblock_region* regions;        // 指向记录各个内存区域的数组
    char*                   name;           //
};
```

struct memblock

```c
/**
 * struct memblock - memblock allocator metadata
 * @bottom_up: is bottom up direction?
 * @current_limit: physical address of the current allocation limit
 * @memory: usable memory regions
 * @reserved: reserved memory regions
 */
struct memblock
{
    bool        bottom_up; /* is bottom up direction? 
                              内存分配方向：是自低向高？ 
                              还是自高向低？*/
    phys_addr_t current_limit;     // 内存分配的物理地址上线
    struct memblock_type memory;   // 管理可用物理内存
    struct memblock_type reserved; // 管理预留内存
};

extern struct memblock memblock;
```

__initdata_memblock

```c
// memblock_memory_init_regions 变量放置到初始化数据区
static struct memblock_region
        memblock_memory_init_regions[INIT_MEMBLOCK_MEMORY_REGIONS] __initdata_memblock;
static struct memblock_region
        memblock_reserved_init_regions[INIT_MEMBLOCK_RESERVED_REGIONS] __initdata_memblock;

// 定义变量 memblock 放置到初始化数据区
// 确保数据在初始化完成后会被销毁或者覆盖，节省内存
struct memblock memblock __initdata_memblock = {
    .memory.regions   = memblock_memory_init_regions,
    .memory.max       = INIT_MEMBLOCK_MEMORY_REGIONS,
    .memory.name      = "memory",

    .reserved.regions = memblock_reserved_init_regions,
    .reserved.max     = INIT_MEMBLOCK_RESERVED_REGIONS,
    .reserved.name    = "reserved",

    .bottom_up        = false,
    .current_limit    = MEMBLOCK_ALLOC_ANYWHERE,
};
```

## x86 BIOS报告内存信息三大机制

### BIOS-e820

通过 `INT 0x15` 中断的 `AX=0xE820` 操作码获取内存布局，报告物理内存的详细区域（如可用内存、保留内存、ACPI 等）。其数据结构包含起始地址、长度和类型，是现代 Linux 内核默认使用的内存探测方式。例如，内核启动时会通过 e820 中断获取内存信息并打印到日志中（如 dmesg | grep e820）

中断调用：BIOS-e820 通过 `INT 0x15` 中断与 BIOS 通信，设置 AX 寄存器为 `0xE820`，EDX 寄存器为 "SMAP" 标志（`0x534D4150`），`ES:DI` 指向缓冲区接收数据。
数据结构：每次中断返回一个内存区域描述符（20 字节），包含：
- 基地址（Base Address）：内存区域的起始地址（64 位）。
- 长度（Size）：内存区域的大小（64 位）。
- 类型（Type）：标识内存用途（如 `E820_RAM` 表示可用内存，`E820_RESERVED` 表示保留内存）。

### BIOS-88

通过 `INT 15h` 中断的 `AX=0x88` 操作码获取扩展内存（Extended Memory）的大小，仅返回一个 16 位的值（最大 64MB），适用于早期 DOS 系统。在 Linux 内核中，若 BIOS-e820 报告的内存不足，会回退使用 BIOS-88 的值（`boot_params.screen_info.ext_mem_k`）计算物理内存总量。

### BIOS-e801

通过 `INT 15h` 中断的 `AX=0xE801` 操作码报告内存的低 1MB（Base Memory）和扩展内存（Extended Memory）大小。其返回值包含两个 16 位值：`alt_mem_k`（低 1MB 内存）和 `ext_mem_k`（扩展内存）。在 Linux 内核中，若 BIOS-e820 失败，会优先使用 BIOS-e801 的 `ext_mem_k` 作为物理内存大小

### 总结

- BIOS-e820 是最全面的内存报告机制，支持现代内存类型和分区
- BIOS-88 和 BIOS-e801 是旧版 BIOS 的遗留接口，分别用于获取扩展内存和基础内存信息。内核会根据探测结果选择最可靠的内存总量，优先使用 BIOS-e820，其次为 BIOS-e801 或 BIOS-88

## PTI机制

PTI(Page Table Isolation, 页表隔离)是 Linux 内核引入的一种安全机制，旨在防止 CPU 旁道攻击(如：meltdown漏洞)影响内核数据的安全性。

PTI 主要在x86_64架构上启用，通过将用户态和内核态的页表完全隔离，防止用户态进程利用CPU的缓存投机执行来读取内核内存数据。

在传统的 linux 内存管理中，内核地址空间始终映射在所有进程的页表中(只是 CR3 控制位上设置不可访问)。恶意进程可以利用投机执行访问这些地址，借助缓存侧信道泄露内核数据。

### PTI如何解决Meltdown

- 用户态进程的页表不再包含内核页表，彻底杜绝了用户态访问任何内核地址的可能性
- 用户态进程切换到内核态时候，才会加载完整的内核页表(通过CR3重新加载页表)

### 关键机制

两个页表
- 用户页表(User CR3)：只包含用户态映射，不包含内核页表。
- 内核页表(Kernel CR3)：包含完整的用户和内核映射，只有在切换到内核态时候才使用

#### 页表切换

- 用户态执行时候，CR3只加载用户页表(隔离内核地址)
- 进入内核态（例如系统调用或中断）时候：
    - CPU切换到完整的内核页表(CR3加载内核页表)
    - 处理完系统调用后，切换回用户页表

#### 影子页表

PTI机制下，内核维护影子页表(shadow page tables)：
- 用户态影子页表：不包含内核地址，只在用户态可用
- 内核态影子页表：完整页表，处理系统调用等。

### PTI的影响

#### 性能影响

- 页表切换开销增加：每次系统调用中断需要切换CR3，带来TLB刷新，影响性能(特别是频繁系统调用)。
- 影响I/O密集型应用：数据库、网络大量依赖系统调用

#### 针对性能影响的优化

- PCID(Process Context ID)：减少 CR3 切换导致的TLB刷新开销
- INVPCID指令(仅支持部分CPU)：部分 TLB 条目无效化，优化切换开销。
